{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Customer Churn Prediction (AWS Mini-Project)\n",
        "\n",
        "Sections: Data Loading → EDA → Feature Engineering → Baseline Models → Improved Model → Evaluation → Threshold Selection → Feature Importance → (Optional) AWS SageMaker Deploy → Conclusions\n",
        "\n",
        "Links:\n",
        "- Project brief: https://github.com/springboard-curriculum/mec2-projects/blob/main/Student_MLE_MiniProject_Churn_Prediction_AWS.md\n",
        "- AWS reference: https://aws.amazon.com/blogs/machine-learning/build-tune-and-deploy-an-end-to-end-churn-prediction-model-using-amazon-sagemaker-pipelines/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Loading\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "\n",
        "pd.set_option('display.max_columns', 200)\n",
        "\n",
        "# TODO: set your dataset path\n",
        "CSV_PATH = Path('data/churn.csv')\n",
        "if CSV_PATH.exists():\n",
        "    df = pd.read_csv(CSV_PATH)\n",
        "else:\n",
        "    # fallback tiny demo\n",
        "    df = pd.DataFrame({\n",
        "        'customer_id': range(1, 11),\n",
        "        'tenure_months': [1,3,6,12,24,36,48,60,72,84],\n",
        "        'monthly_spend': [35,40,38,45,50,60,65,70,80,90],\n",
        "        'churn': [1,0,1,0,0,0,1,0,0,0],\n",
        "        'plan': ['basic','basic','plus','plus','pro','pro','pro','plus','pro','basic']\n",
        "    })\n",
        "\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EDA — churn rate & class balance\n",
        "assert 'churn' in df.columns, \"Dataset must include 'churn' column (0/1).\"\n",
        "churn_rate = df['churn'].mean()\n",
        "print(f\"Churn rate: {churn_rate:.3f}\")\n",
        "\n",
        "# Class counts plot\n",
        "ax = df['churn'].value_counts().sort_index().plot(kind='bar', color=['#4daf4a','#e41a1c'])\n",
        "ax.set_xticklabels(['No churn (0)','Churn (1)'], rotation=0)\n",
        "ax.set_title('Class Balance')\n",
        "plt.show()\n",
        "\n",
        "# Basic stats\n",
        "display(df.describe(include='all'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature schema & utilities\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import roc_auc_score, precision_recall_curve, roc_curve, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "TARGET = 'churn'\n",
        "assert TARGET in df.columns, f\"Target '{TARGET}' missing\"\n",
        "\n",
        "# Identify feature types (simple heuristic)\n",
        "num_cols = [c for c in df.columns if df[c].dtype != 'O' and c not in [TARGET]]\n",
        "cat_cols = [c for c in df.columns if df[c].dtype == 'O']\n",
        "\n",
        "# Example engineered feature(s)\n",
        "if 'monthly_spend' in df.columns:\n",
        "    df['annual_spend'] = df['monthly_spend'] * 12\n",
        "    if 'annual_spend' not in num_cols: num_cols.append('annual_spend')\n",
        "if 'tenure_months' in df.columns:\n",
        "    df['tenure_bucket'] = pd.cut(df['tenure_months'], bins=[-1,3,12,36,120], labels=['new','early','mid','long'])\n",
        "    if 'tenure_bucket' not in cat_cols: cat_cols.append('tenure_bucket')\n",
        "\n",
        "X = df[num_cols + cat_cols].copy()\n",
        "y = df[TARGET].astype(int).copy()\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler(with_mean=False))\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
        "])\n",
        "\n",
        "preprocess = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, num_cols),\n",
        "        ('cat', categorical_transformer, cat_cols)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Helper to evaluate and plot\n",
        "def evaluate(name, model, X_test, y_test, proba=None):\n",
        "    if proba is None:\n",
        "        if hasattr(model, 'predict_proba'):\n",
        "            proba = model.predict_proba(X_test)[:,1]\n",
        "        elif hasattr(model, 'decision_function'):\n",
        "            from sklearn.preprocessing import MinMaxScaler\n",
        "            scores = model.decision_function(X_test).reshape(-1,1)\n",
        "            proba = MinMaxScaler().fit_transform(scores).ravel()\n",
        "        else:\n",
        "            proba = model.predict(X_test)\n",
        "    preds = (proba >= 0.5).astype(int)\n",
        "    auc = roc_auc_score(y_test, proba)\n",
        "    prec = precision_score(y_test, preds, zero_division=0)\n",
        "    rec = recall_score(y_test, preds, zero_division=0)\n",
        "    f1 = f1_score(y_test, preds, zero_division=0)\n",
        "    cm = confusion_matrix(y_test, preds)\n",
        "    print(f\"[{name}] AUC={auc:.3f}  Precision={prec:.3f}  Recall={rec:.3f}  F1={f1:.3f}\")\n",
        "    print(\"Confusion Matrix:\\n\", cm)\n",
        "    # ROC\n",
        "    fpr, tpr, _ = roc_curve(y_test, proba)\n",
        "    plt.figure(figsize=(5,4)); plt.plot(fpr,tpr,label=f'{name} (AUC={auc:.2f})'); plt.plot([0,1],[0,1],'--',color='gray'); plt.xlabel('FPR'); plt.ylabel('TPR'); plt.title('ROC'); plt.legend(); plt.show()\n",
        "    # PR\n",
        "    precs, recs, _ = precision_recall_curve(y_test, proba)\n",
        "    plt.figure(figsize=(5,4)); plt.plot(recs,precs,label=name); plt.xlabel('Recall'); plt.ylabel('Precision'); plt.title('PR Curve'); plt.legend(); plt.show()\n",
        "    return {\"auc\":auc,\"precision\":prec,\"recall\":rec,\"f1\":f1}\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Baselines: Logistic Regression & Decision Tree\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "logreg = make_pipeline(preprocess, LogisticRegression(max_iter=1000, n_jobs=None))\n",
        "logreg.fit(X_train, y_train)\n",
        "res_logreg = evaluate('LogReg', logreg, X_test, y_test)\n",
        "\n",
        "# simple tree\n",
        "cart = make_pipeline(preprocess, DecisionTreeClassifier(max_depth=6, random_state=42))\n",
        "cart.fit(X_train, y_train)\n",
        "res_cart = evaluate('DecisionTree', cart, X_test, y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Improved model: XGBoost (if available) else RandomForest\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "    xgb = make_pipeline(preprocess, XGBClassifier(\n",
        "        n_estimators=400, learning_rate=0.05, max_depth=5, subsample=0.9, colsample_bytree=0.8, random_state=42, eval_metric='auc'\n",
        "    ))\n",
        "    xgb.fit(X_train, y_train)\n",
        "    res_xgb = evaluate('XGBoost', xgb, X_test, y_test)\n",
        "except Exception as e:\n",
        "    print('XGBoost not available:', e)\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    rf = make_pipeline(preprocess, RandomForestClassifier(n_estimators=300, max_depth=None, random_state=42))\n",
        "    rf.fit(X_train, y_train)\n",
        "    res_xgb = evaluate('RandomForest', rf, X_test, y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Threshold selection & business sketch\n",
        "# Sweep thresholds and compute precision/recall\n",
        "model = xgb if 'xgb' in globals() else (rf if 'rf' in globals() else logreg)\n",
        "if hasattr(model, 'predict_proba'):\n",
        "    proba = model.predict_proba(X_test)[:,1]\n",
        "else:\n",
        "    proba = model.predict(X_test).astype(float)\n",
        "\n",
        "ths = np.linspace(0.1, 0.9, 9)\n",
        "rows = []\n",
        "for t in ths:\n",
        "    preds = (proba >= t).astype(int)\n",
        "    rows.append({\n",
        "        'threshold': t,\n",
        "        'precision': precision_score(y_test, preds, zero_division=0),\n",
        "        'recall': recall_score(y_test, preds, zero_division=0),\n",
        "        'f1': f1_score(y_test, preds, zero_division=0)\n",
        "    })\n",
        "import pandas as pd\n",
        "thr_df = pd.DataFrame(rows)\n",
        "thr_df\n",
        "\n",
        "# Simple business cost sketch:\n",
        "# cost_retention per contacted customer, value_recovered per true churn caught\n",
        "cost_retention = 5.0\n",
        "value_recovered = 50.0\n",
        "thr_df['expected_value'] = thr_df['recall']*value_recovered - (thr_df['precision']*cost_retention)\n",
        "thr_df.sort_values('expected_value', ascending=False).head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature importance (tree-based) and optional SHAP\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "    base_est = model.named_steps.get('xgbclassifier') if hasattr(model, 'named_steps') else None\n",
        "    if base_est is None and isinstance(model, XGBClassifier): base_est = model\n",
        "    if base_est is not None:\n",
        "        # importance in original feature space after preprocess is complex;\n",
        "        # show model-native importance as a proxy\n",
        "        importances = getattr(base_est, 'feature_importances_', None)\n",
        "        if importances is not None:\n",
        "            imp = pd.Series(importances).sort_values(ascending=False)[:20]\n",
        "            imp.plot(kind='bar', title='Top model feature importances (model space)'); plt.show()\n",
        "except Exception as e:\n",
        "    print('Feature importance skipped:', e)\n",
        "\n",
        "# Optional SHAP (if available)\n",
        "try:\n",
        "    import shap\n",
        "    shap.initjs()\n",
        "    # Use a small background sample due to cost\n",
        "    X_small = X_train.sample(min(200, len(X_train)), random_state=42)\n",
        "    # Get transformed features\n",
        "    X_small_tx = preprocess.fit(X_train).transform(X_small)\n",
        "    if 'xgbclassifier' in model.named_steps:\n",
        "        explainer = shap.TreeExplainer(model.named_steps['xgbclassifier'])\n",
        "        shap_values = explainer.shap_values(X_small_tx)\n",
        "        shap.summary_plot(shap_values, X_small_tx, show=True)\n",
        "except Exception as e:\n",
        "    print('SHAP skipped:', e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusions\n",
        "\n",
        "- Recommended model: fill in XGBoost/RandomForest/LogReg based on AUC/PR results above.\n",
        "- Threshold choice: pick threshold that maximizes expected_value (see threshold table) or balances precision/recall for your business.\n",
        "- Next steps: add cross-validation; persist model with joblib; (optional) wire SageMaker Pipelines for S3 upload, training, HPO, RegisterModel, and batch transform.\n",
        "- Monitoring: track p95 latency and error rate on inference; monitor drift via class priors and periodic AUC/PR checks on a labeled trickle sample.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
