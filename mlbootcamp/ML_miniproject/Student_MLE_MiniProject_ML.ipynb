{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Project: Trees and Forests\n\nWorking with decision trees, random forests, and AdaBoost on the breast cancer dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n\nFirst let's load the breast cancer dataset and split it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "cancer_data = load_breast_cancer()\n",
    "df = pd.DataFrame(cancer_data.data, columns=cancer_data.feature_names)\n",
    "df['target'] = cancer_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/test\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>...</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n0        17.99         10.38          122.80     1001.0          0.11840   \n1        20.57         17.77          132.90     1326.0          0.08474   \n2        19.69         21.25          130.00     1203.0          0.10960   \n3        11.42         20.38           77.58      386.1          0.14250   \n4        20.29         14.34          135.10     1297.0          0.10030   \n\n   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n0           0.27760          0.30010              0.14710        0.2419   \n1           0.07864          0.08690              0.07017        0.1812   \n2           0.15990          0.19740              0.12790        0.2069   \n3           0.28390          0.24140              0.10520        0.2597   \n4           0.13280          0.19800              0.10430        0.1809   \n\n   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n0                0.07871  ...         25.38          17.33           184.60   \n1                0.05667  ...         24.99          23.41           158.80   \n2                0.05999  ...         23.57          25.53           152.50   \n3                0.09744  ...         14.91          26.50            98.87   \n4                0.05883  ...         22.54          16.67           152.20   \n\n   worst area  worst smoothness  worst compactness  worst concavity  \\\n0      2019.0           0.16220             0.66560          0.71190   \n1      1956.0           0.12380             0.18660          0.24160   \n2      1709.0           0.14440             0.42450          0.45040   \n3       567.7           0.20980             0.86630          0.68690   \n4      1575.0           0.13740             0.20500          0.40000   \n\n   worst concave points  worst symmetry  worst fractal dimension  target  \n0               0.26540          0.4601                0.11890       0  \n1               0.18600          0.2750                0.08902       0  \n2               0.24300          0.3613                0.08758       0  \n3               0.25750          0.6638                0.17300       0  \n4               0.16250          0.2364                0.07678       0  \n\n[5 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "outputs": []
    }
   ],
   "source": [
    "# Check the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n\nLet's try a decision tree classifier first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a decision tree\n",
    "tree_model = DecisionTreeClassifier(random_state=99)\n",
    "tree_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the tree structure\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(tree_model, \n",
    "         feature_names=X.columns, \n",
    "         class_names=['Malignant', 'Benign'], \n",
    "         filled=True, \n",
    "         rounded=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different max_depth values to see the effect\n",
    "depths = [3, 5, 10, None]\n",
    "for depth in depths:\n",
    "    tree_model_tuned = DecisionTreeClassifier(max_depth=depth, random_state=99)\n",
    "    tree_model_tuned.fit(X_train, y_train)\n",
    "    train_score = tree_model_tuned.score(X_train, y_train)\n",
    "    test_score = tree_model_tuned.score(X_test, y_test)\n",
    "    print(f\"Max Depth: {depth}, Train Score: {train_score:.3f}, Test Score: {test_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n\nNow let's try a random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a random forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=99)\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check feature importances\n",
    "feature_importances = dict(zip(X.columns, rf_model.feature_importances_))\n",
    "sorted_importances = dict(sorted(feature_importances.items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top features\n",
    "sorted_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importances\n",
    "plt.figure(figsize=(12, 8))\n",
    "features = list(sorted_importances.keys())[:15]\n",
    "importances = list(sorted_importances.values())[:15]\n",
    "\n",
    "plt.barh(range(len(features)), importances)\n",
    "plt.yticks(range(len(features)), features)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 15 Feature Importances')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost\n\nNow let's try AdaBoost with a weak learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train AdaBoost\n",
    "ada_model = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1, random_state=99),\n",
    "    n_estimators=100,\n",
    "    random_state=99\n",
    ")\n",
    "ada_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "AdaBoost Accuracy: 97.37%\n",
      "Tree Accuracy: 94.74%\n",
      "RF Accuracy: 96.49%\n"
     ]
    }
   ],
   "source": [
    "# Compare all models\n",
    "y_pred_ada = ada_model.predict(X_test)\n",
    "accuracy_ada = accuracy_score(y_test, y_pred_ada)\n",
    "print(f\"AdaBoost Accuracy: {accuracy_ada * 100:.2f}%\")\n",
    "\n",
    "y_pred_tree = tree_model.predict(X_test)\n",
    "accuracy_tree = accuracy_score(y_test, y_pred_tree)\n",
    "print(f\"Tree Accuracy: {accuracy_tree * 100:.2f}%\")\n",
    "\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"RF Accuracy: {accuracy_rf * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n\nAdaBoost performed best with 97.37% accuracy, followed by Random Forest at 96.49%, and the single decision tree at 94.74%.\n\nThe ensemble methods (Random Forest and AdaBoost) performed better than the single decision tree, which makes sense since they combine multiple models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try cross-validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "print(\"Cross-validation scores (5-fold):\")\n",
    "print(f\"Decision Tree: {cross_val_score(tree_model, X, y, cv=5).mean():.3f} (+/- {cross_val_score(tree_model, X, y, cv=5).std() * 2:.3f})\")\n",
    "print(f\"Random Forest: {cross_val_score(rf_model, X, y, cv=5).mean():.3f} (+/- {cross_val_score(rf_model, X, y, cv=5).std() * 2:.3f})\")\n",
    "print(f\"AdaBoost: {cross_val_score(ada_model, X, y, cv=5).mean():.3f} (+/- {cross_val_score(ada_model, X, y, cv=5).std() * 2:.3f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}